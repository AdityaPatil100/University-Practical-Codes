{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28c25c18",
   "metadata": {
    "id": "28c25c18",
    "outputId": "2c6b7e45-5e9a-4f28-9452-e9eae1a92b35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading averaged-perceptron-tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10060] A connection attempt failed because\n",
      "[nltk_data]     the connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged-perceptron-tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98a5c35d",
   "metadata": {
    "id": "98a5c35d"
   },
   "outputs": [],
   "source": [
    "text=\"Tokenization is the first step in text analytics.The process of breaking down a text paragraph into small words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c0acca7",
   "metadata": {
    "id": "2c0acca7",
    "outputId": "81995275-8723-491d-cb48-747a0df53521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is the first step in text analytics.The process of breaking down a text paragraph into small words']\n"
     ]
    }
   ],
   "source": [
    "#Perform Tokenization\n",
    "#Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text= sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3611460a",
   "metadata": {
    "id": "3611460a",
    "outputId": "5a332628-a371-44a5-8a4e-1d368f095ace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics.The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'small', 'words']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8d32388",
   "metadata": {
    "id": "b8d32388",
    "outputId": "c67c74ce-0ad8-4a9d-974d-ae666ed8bafd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'there', 'ourselves', 'then', 'he', 'myself', 'and', 'in', 'what', 'its', 'not', 'd', 'with', 'where', 'couldn', 'himself', 'an', \"couldn't\", 'were', 'above', 'mustn', 'have', 'wasn', 'her', 'each', 'was', 'won', \"aren't\", 'll', 'all', 'why', 'about', 'on', 'doesn', 'against', 'does', 'should', 'i', 'below', 'o', \"won't\", \"wouldn't\", 'am', 'didn', 'their', 'for', \"hasn't\", 'by', \"isn't\", \"wasn't\", 'wouldn', 'she', 'ma', 'those', 'the', 'to', 'be', 'had', 'too', 'my', 'than', 've', 'up', 'which', \"that'll\", 'we', 'do', 'when', 'shouldn', \"you're\", 'who', 'it', 'during', 'between', 'our', 'weren', 'm', 'having', 'yourselves', 'from', 'both', 'only', \"you'll\", 'some', 'before', 'other', 'here', \"needn't\", 'y', 'these', \"shan't\", 'they', 'once', 'your', 'any', 'until', 'no', \"mightn't\", 'you', 'very', \"you'd\", 'this', 'nor', 'after', 'mightn', 'shan', 'that', 'down', 'while', \"she's\", \"doesn't\", 'aren', 'themselves', 'has', 'how', 'because', \"should've\", 'yours', 'isn', 'did', 'few', 'into', \"it's\", 'don', 'is', 'him', 'hadn', 'needn', 'as', 'of', 'at', \"mustn't\", 'most', \"you've\", 'again', 're', \"weren't\", 'over', 's', 'been', 'further', 'his', 'will', 'herself', 'now', 'under', \"shouldn't\", 'more', 'off', 'through', 'me', 'can', 'are', \"didn't\", 'ain', 'them', 't', 'whom', 'so', 'out', \"don't\", 'doing', 'same', 'if', 'or', 'a', \"hadn't\", 'own', 'hasn', 'but', \"haven't\", 'yourself', 'ours', 'hers', 'being', 'such', 'itself', 'just', 'theirs', 'haven'}\n"
     ]
    }
   ],
   "source": [
    "#Removing Punctuations and Stop Word\n",
    "# print stop words of English\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5476aa2b",
   "metadata": {
    "id": "5476aa2b",
    "outputId": "74e0651c-20bd-490f-9a2c-7cc1ed34662d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to remove stop words with nltk library in python '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text ='How to remove stop words with nltk library in python?'\n",
    "import re\n",
    "text= re.sub('[^a-zA-Z]', ' ',text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b503aca",
   "metadata": {
    "id": "0b503aca",
    "outputId": "be5f5a31-69bd-44b4-bcd5-d06531ba8979"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'to',\n",
       " 'remove',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'with',\n",
       " 'nltk',\n",
       " 'library',\n",
       " 'in',\n",
       " 'python']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text.lower())\n",
    "filtered_text=[]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1838d157",
   "metadata": {
    "id": "1838d157",
    "outputId": "d6a8464d-5aa6-4b63-f14b-e4027dc6e373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
      "Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
     ]
    }
   ],
   "source": [
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_text.append(w)\n",
    "print(\"Tokenized Sentence:\",tokens)\n",
    "print(\"Filterd Sentence:\",filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bfe2b38",
   "metadata": {
    "id": "5bfe2b38",
    "outputId": "430c886c-2a30-4cd2-a2de-e45bafa716c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studi\n",
      "studi\n",
      "cri\n",
      "cri\n"
     ]
    }
   ],
   "source": [
    "#Perform Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"studies\",\"studying\",\"cries\",\"cry\"]\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b302f3d",
   "metadata": {
    "id": "5b302f3d",
    "outputId": "caa412c6-fe89-4b76-a359-f5c508c9bfc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "#Perform Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a248a87",
   "metadata": {
    "id": "9a248a87",
    "outputId": "bf3d622b-fe68-4d55-d6c6-29959e9921d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "#Perform Lemmatization(STEMMING AND LEMANIZATION ARE DIFFERENT) IT IS MORE BENEFICIAL THAN STEMMING\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w,wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da675c0f",
   "metadata": {
    "id": "da675c0f"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\adity/nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe blue sweater fit her perfectly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m words\u001b[38;5;241m=\u001b[39mword_tokenize(data)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\u001b[38;5;28mprint\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mpos_tag([word]))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mpos_tag([word]))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\adity/nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data=\"The blue sweater fit her perfectly\"\n",
    "words=word_tokenize(data)\n",
    "for word in words:print(nltk.pos_tag([word]))\n",
    "print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574863e7",
   "metadata": {},
   "source": [
    "# PART B\n",
    "Create representation of document by calculating TFIDF\n",
    "##TF = (NO. OF OCCURANCE OF W)/(TOTAL NO.OF WORDS IN DOCUMNET)\n",
    "##IDF = (TOTAL NO.OF DOCUMNET)/(NO. OF DOCUMNET CONTAING W)\n",
    "##TFIDF = TF*IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f7bf1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32977a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the Documents.\n",
    "documentA = 'Jupiter is the largest Planet'\n",
    "documentB = 'Mars is the fourth planet from the Sun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e366176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create BagofWords (BoW) for Document A and B. CONVERT THE TEXT INTO NO.OF WORDS\n",
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beaf4e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jupiter',\n",
       " 'Mars',\n",
       " 'Planet',\n",
       " 'Sun',\n",
       " 'fourth',\n",
       " 'from',\n",
       " 'is',\n",
       " 'largest',\n",
       " 'planet',\n",
       " 'the'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Collection of Unique words from Document A and B.\n",
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "uniqueWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6a4e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary of words and their occurrence for each document in the\n",
    "#corpus\n",
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    " numOfWordsA[word] += 1\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    " numOfWordsB[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d70acf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'Jupiter': 1,\n",
       " 'fourth': 0,\n",
       " 'is': 1,\n",
       " 'Planet': 1,\n",
       " 'planet': 0,\n",
       " 'largest': 1,\n",
       " 'Sun': 0,\n",
       " 'from': 0,\n",
       " 'Mars': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dictionary of words and their occurrence for each document in the\n",
    "#corpus\n",
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    " numOfWordsA[word] += 1\n",
    "numOfWordsA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4015602a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 2,\n",
       " 'Jupiter': 0,\n",
       " 'fourth': 1,\n",
       " 'is': 1,\n",
       " 'Planet': 0,\n",
       " 'planet': 1,\n",
       " 'largest': 0,\n",
       " 'Sun': 1,\n",
       " 'from': 1,\n",
       " 'Mars': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    " numOfWordsB[word] += 1\n",
    "numOfWordsB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3713e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the occurance of each word in the document\n",
    "def computeTF(wordDict, bagOfWords):\n",
    " tfDict = {}\n",
    " bagOfWordsCount = len(bagOfWords)\n",
    " for word, count in wordDict.items():\n",
    "  tfDict[word] = count / float(bagOfWordsCount)\n",
    " return tfDict\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0578b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0.2,\n",
       " 'Jupiter': 0.2,\n",
       " 'fourth': 0.0,\n",
       " 'is': 0.2,\n",
       " 'Planet': 0.2,\n",
       " 'planet': 0.0,\n",
       " 'largest': 0.2,\n",
       " 'Sun': 0.0,\n",
       " 'from': 0.0,\n",
       " 'Mars': 0.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed69aebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0.0,\n",
       " 'Jupiter': 0.6931471805599453,\n",
       " 'fourth': 0.6931471805599453,\n",
       " 'is': 0.0,\n",
       " 'Planet': 0.6931471805599453,\n",
       " 'planet': 0.6931471805599453,\n",
       " 'largest': 0.6931471805599453,\n",
       " 'Sun': 0.6931471805599453,\n",
       " 'from': 0.6931471805599453,\n",
       " 'Mars': 0.6931471805599453}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute the term Inverse Document Frequency.\n",
    "def computeIDF(documents):\n",
    " import math\n",
    " N = len(documents)\n",
    " idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    " for document in documents:\n",
    "  for word, val in document.items():\n",
    "   if val > 0:\n",
    "       idfDict[word] += 1\n",
    " for word, val in idfDict.items():\n",
    "    idfDict[word] = math.log(N / float(val))\n",
    " return idfDict\n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4876fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the term TF/IDF for all words.\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "   tfidf = {}\n",
    "   for word, val in tfBagOfWords.items():\n",
    "     tfidf[word] = val * idfs[word]\n",
    "     return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1396b3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   the\n",
       "0  0.0\n",
       "1  0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "108a653c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\adity/nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe blue sweater fit her perfectly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m words\u001b[38;5;241m=\u001b[39mword_tokenize(data)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\u001b[38;5;28mprint\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mpos_tag([word]))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mpos_tag([word]))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\adity/nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\adity\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data=\"The blue sweater fit her perfectly\"\n",
    "words=word_tokenize(data)\n",
    "for word in words:print(nltk.pos_tag([word]))\n",
    "print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec614b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
